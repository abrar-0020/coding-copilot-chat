import streamlit as st
import google.generativeai as genai
import pdfplumber
import pytesseract
from pdf2image import convert_from_bytes
from PIL import Image
import io
import os
from dotenv import load_dotenv

# ğŸ”¹ Load Gemini API Key
load_dotenv()
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

MODEL_NAME = "gemini-1.5-flash"  # âœ… Fast model

# â”€â”€ Gemini Query â”€â”€
def query_gemini_text(prompt: str) -> str:
    try:
        model = genai.GenerativeModel(MODEL_NAME)
        response = model.generate_content(prompt)
        return response.text.strip()
    except Exception as e:
        return f"âŒ Gemini API Error: {e}"

# â”€â”€ Extract Text from PDF â”€â”€
def extract_text_from_pdf(file_bytes: bytes) -> str:
    text = ""
    try:
        with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:
            for page in pdf.pages[:5]:
                text += page.extract_text() or ""
    except:
        try:
            images = convert_from_bytes(file_bytes)
            for img in images[:3]:
                text += pytesseract.image_to_string(img)
        except:
            text = "âš ï¸ Could not extract text from this PDF."
    return text[:4000] or "âš ï¸ No readable text found in PDF."

# â”€â”€ Detect Code Language from Extension â”€â”€
def detect_language(filename: str) -> str:
    ext = filename.split(".")[-1]
    return {
        "py": "python",
        "java": "java",
        "c": "c",
        "cpp": "cpp",
        "html": "html",
        "css": "css",
        "js": "javascript"
    }.get(ext, "")

# â”€â”€ Streamlit Config â”€â”€
st.set_page_config(page_title="Mini Copilot", page_icon="ğŸ¤–")
st.title("ğŸ¤– Mini Copilot â€“ AI File & Code Assistant")

# Session State
if "messages" not in st.session_state:
    st.session_state.messages = []
if "file_processed" not in st.session_state:
    st.session_state.file_processed = False

# Sidebar
if st.sidebar.button("ğŸ†• New Chat"):
    st.session_state.messages.clear()
    st.session_state.file_processed = False

# Show previous messages
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# â”€â”€ File Upload â”€â”€
uploaded_file = st.file_uploader("ğŸ“‚ Upload any file", type=None)
if uploaded_file and not st.session_state.file_processed:
    file_bytes = uploaded_file.read()
    file_name = uploaded_file.name.lower()
    reply = ""

    # Display uploaded file message
    st.session_state.messages.append({"role": "user", "content": f"ğŸ“„ Uploaded: {uploaded_file.name}"})
    with st.chat_message("user"):
        st.markdown(f"ğŸ“„ Uploaded: **{uploaded_file.name}**")

    # âœ… Handle Source Code Files (py, java, c, cpp, html, css, js)
    if file_name.endswith((".py", ".java", ".c", ".cpp", ".html", ".css", ".js")):
        try:
            file_content = file_bytes.decode("utf-8", errors="ignore")[:4000]
            lang = detect_language(file_name)
            if lang:
                st.code(file_content, language=lang)
            else:
                st.text_area("ğŸ“„ Source Code", file_content, height=200)
            prompt = f"Explain this {lang.upper() if lang else 'source code'} in simple terms:\n```{lang}\n{file_content}\n```"
            reply = query_gemini_text(prompt)
        except Exception as e:
            reply = f"âŒ Could not read this code file: {e}"

    # âœ… Handle PDF Files
    elif file_name.endswith(".pdf"):
        pdf_text = extract_text_from_pdf(file_bytes)
        st.text_area("ğŸ“„ Extracted PDF Text", pdf_text, height=200)
        prompt = f"Summarize and explain this PDF:\n{pdf_text}"
        reply = query_gemini_text(prompt)

    # âœ… Handle Images
    else:
        try:
            img = Image.open(io.BytesIO(file_bytes))
            st.image(img, caption="ğŸ“¸ Uploaded Image", use_column_width=True)
            model = genai.GenerativeModel(MODEL_NAME)
            response = model.generate_content(["Describe this image in detail.", img])
            reply = response.text.strip()
        except:
            reply = query_gemini_text("This is a binary file. Explain what it may contain.")

    # Show AI Reply
    st.session_state.messages.append({"role": "assistant", "content": reply})
    with st.chat_message("assistant"):
        st.markdown(reply)

    st.session_state.file_processed = True

# â”€â”€ User Prompt â”€â”€
user_input = st.chat_input("Ask something about your file or code...")
if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    with st.chat_message("assistant"):
        placeholder = st.empty()
        placeholder.markdown("ğŸ’­ Thinking...")
        reply = query_gemini_text(user_input)
        placeholder.markdown(reply)

    st.session_state.messages.append({"role": "assistant", "content": reply})
